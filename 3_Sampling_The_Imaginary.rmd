---
title: "Chapter 3 -- Sampling The Imaginary"
output: 
  pdf_document: 
    fig_caption: yes
---

```{r setup, include=F}
# knitr::opts_chunk$set(echo = FALSE)
rm(list=ls())
```

## 3.1  Sampling from a grid-approximate posterior

* R Code 3.2:
```{r}
n = 1000
p_grid <- seq(from=0, to=1, length.out=n)
prior <- rep(1, n)
likelihood <- dbinom(x=6, size=9, prob=p_grid)
posterior_notnorm <- likelihood * prior
posterior <- posterior_notnorm / sum(posterior_notnorm)
```

Draw 10,000 samples:
* R Code 3.3:
```{r}
samples_orig <- sample(p_grid, prob=posterior, size=n, replace=T)
samples = samples_orig
```

* 3.4:
```{r}
plot(samples)
```

```{r include=F}
library(rethinking)
```

* 3.5:
```{r}
dens(samples)
```

Let's try more samples:

```{r}
par(mfrow=c(2, 2))
dens(sample(p_grid, prob=posterior, size=1e3, replace=T))
dens(sample(p_grid, prob=posterior, size=1e4, replace=T))
dens(sample(p_grid, prob=posterior, size=1e5, replace=T))
dens(sample(p_grid, prob=posterior, size=1e6, replace=T))
```

## 3.2 Sampling to Summarize

### 3.2.1. Intervals of defined boundaries.

The posterior probability that the proportion of water is less than 0.5:

* 3.6:
```{r}
p_grid < 0.5
sum(posterior[p_grid < 0.5])
```

Samples array:
```{r}
head(samples, 100)
```

The same calculation using samples. Add up all samples that lie in the grid < 0.5, and divide by the total number of samples to get the frequency ~ probability:

* 3.7:
```{r}
n = 1e4
samples = sample(p_grid, prob=posterior, size=n, replace=T)
sum(samples < 0.5) / n
```

 How much probability lies between 0.5 and 0.75:
 * 3.8:
```{r}
sample_points = sum(samples > 0.5 & samples < 0.75) 
sample_points
sample_points / n

```

### 3.2.2. Intervals of defined mass.

Boundaries of the lower 80% posterior probability lies:

* 3.9:
```{r}
quantile(samples, probs = .8)
```

Middle 80%, i.e. lying between 10% and 90%:

```{r}
# 3.10
quantile(samples, probs = c(0.1, 0.9))
```

The above are PERCENTILE INTERVALS. Percentiles can be misleading if the distribution is highly skewed.
```{r}
# 3.11
n <- 10000
p_grid <- seq(0, 1, length.out = n)
prior <- rep(1, n)
likelihood <- dbinom(3, size=3, prob=p_grid)
posterior_notnorm <- likelihood * prior
posterior <- posterior_notnorm / sum(posterior_notnorm)
samples <- sample(p_grid, size=1e5, replace=T, prob=posterior)
plot(posterior, type='l')
```

```{r}
# 3.12
PI(samples, prob=0.5)
```

Highest Posterior Density Interval described the distribution better. It's the *narrowest* interval containing the specified probability mass, e.g. 50%.


```{r}
# 3.13
HPDI(samples, prob=0.5)
```

### 3.2.3. Point Estimates
 
A parameter with the highest posterior probability is called *a maximum a posteriori* estimate, or *MAP*.
```{r}
# 3.14
which.max(posterior)
p_grid[which.max(posterior)]
```

Use samples to get the same (or similar) result:
```{r}
# 3.15
chainmode(samples, adj=0.01)
```

```{r}
# 3.16
mean(samples)
median(samples)
```

If the loss function is the absolute difference, then the posterior loss for p = 0.5 is 
```{r}
# 3.17
sum(posterior * abs(0.5 - p_grid))
```

```{r}
# 3.18
loss <- sapply(p_grid, function(d) sum(posterior * abs(d - p_grid)))
```

```{r}
# 3.19
which.min(loss)
p_grid[which.min(loss)]
```

The posterior median minimizes the abs loss function.
Let's test the quadratic loss function:

```{r}
loss2 <- sapply(p_grid, function(d) sum(posterior * (d - p_grid)^2))
which.min(loss2)
p_grid[which.min(loss2)]
```

This is a mean.

## 3.3. Sampling to Simulate Prediction

### 3.3.1. Dummy Data

```{r}
# 3.20
dbinom(0:2, size=2, prob=0.7)
```

We can sample from this distribution:
```{r}
# 3.22
rbinom(10, size=2, prob=0.7)
```

Let's generate 100,000 dummy observations to verify that each values 0, 1, and 2 appear in proportion to its likelihood:
```{r}
# 3.23
dummy_w <- rbinom(1e5, size=2, prob=0.7)
table(dummy_w) / 1e5
```

Let's simulate the sample with 9 tosses:
```{r}
# 3.24
dummy_w <- rbinom(1e5, size=9, prob=0.7)
plot(table(dummy_w), xlab="Dummy Water Count")
table(dummy_w)
dummy_w[1:100]
```

## 3.2. Model Checking

Below is a misleading distribution plot. While p = 0.6 is the likeliest estimate, if we simply use it as a point estimate we will obtain a much more narrow, "overly confident" predictions:

```{r}
# 3.25
w <- rbinom(1e4, size=9, prob=0.6)
plot(table(w), xlab="Dummy Water Count")
```
The correct way to generate predictions is to incorporate our uncertainty about p. We can do it by using sampled values of p, and averaging over all of them. The sampled values will appear with the right frequency, described by our posterior. Samples from the *posterior* distribution of p:
```{r}
samples[1:20]
```

```{r}
# 3.26
w2 <- rbinom(1e4, size=9, prob=samples_orig)
table(w2)
par(mfrow=c(1, 2))
plot(table(w), xlab ="Overly confident")
plot(table(w2), xlab="Correct")
```

