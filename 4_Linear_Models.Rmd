---
title: "4 - Linear Models"
output: 
  pdf_document: 
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(rethinking)
```

### 4.1.1. Normal by addition 

```{r}
# 4.1
pos <- replicate(1000, sum(runif(16, -1, 1)))
par(mfrow=c(1, 2))
dens(pos, norm.comp = T)
dens(replicate(10000, sum(runif(256, -1, 1))), norm.comp = T)
```

### 4.1.2. Normal by multiplication

```{r}
# 4.2
dens(replicate(1e4, prod(1 + runif(12, 0, 0.1))), norm.comp = T)
```

```{r}
# 4.4
big <- replicate(1e4, prod(1 + runif(12, 0, 0.5)))
small <- replicate(1e4, prod(1 + runif(12, 0, 0.01)))
par(mfrow=c(1, 2))
dens(big, norm.comp = T, main = "Big")
dens(small, norm.comp = T, main = "Small")
```

### Normal by log-multiplication

```{r}
# 4.5
big <- replicate(1e4, prod(1 + runif(12, 0, 1)))
log_big <- log(big)
par(mfrow=c(1, 2))
dens(big, norm.comp = T, main = "Big")
dens(log_big, norm.comp = T, main = "log(Big)")
```

## 4.3 A Gaussian model of height
```{r}
# 4.7
library(rethinking)
data(Howell1)
d <- Howell1
```

```{r}
# 4.8
str(d)
```

We want heights of adults only (352 rows):
```{r}
# 4.10
d2 <- d[d$age >= 18, ]
```

### 4.3.2 The model

Height mean:

```{r}
# 4.11
curve(dnorm(x, 178, 20), from=100, to=250)
```

Height standard deviation:
```{r}
# 4.12
curve(dunif(x, 0, 50), from=10, to=60)
```

```{r}
# 4.13
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```

```{r}
# 4.14
mu_list <- seq(from=140, to=160, length.out=200)
sigma_list <- seq(from=4, to=9, length.out=200)
post <- expand.grid(mu=mu_list, sigma=sigma_list)
post_ll <- sapply(1:nrow(post), function(i) sum(dnorm(
  d2$height,
  mean=post$mu[i],
  sd=post$sigma[i],
  log=T
)))
post_prob <- post_ll + dnorm(post$mu, 178, 20, T) + dunif(post$sigma, 0, 50, T)
plot(post_prob, type="l")
post_prob <- exp(post_prob - max(post_prob))
```


```{r}
# 4.15
contour_xyz(post$mu, post$sigma, post_prob)
```

```{r}
# 4.16
image_xyz(post$mu, post$sigma, post_prob)
```

### 4.3.4 Sampling from the posterior

```{r}
# 4.17
sample.rows <- sample(1:nrow(post), size=1e4, replace = T, prob = post_prob)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
```

```{r}
# 4.18
smoothScatter(sample.mu, sample.sigma, cex=0.5, pch=16, col=col.alpha(rangi2, 0.1))
```
```{r}
# 4.19
dens(sample.mu)
dens(sample.sigma)
```

```{r}
# 4.20
HPDI(sample.mu)
HPDI(sample.sigma)
```

### Smaller Sample
To illustrate the posterior is not always Guassian in shape.

```{r}
# 4.22
d3 <- sample(d2$height, size=10)

small.post_ll <- sapply(1:nrow(post), function(i) sum(dnorm(d3, mean=post$mu[i], sd=post$sigma[i], log=T)))

small.post_product <- small.post_ll + dnorm(post$mu, 178, 20, T) + dunif(post$sigma, 0, 50, T)

small.post_proba <- exp(small.post_product - max(small.post_product))

small.sample.rows <- sample(1:nrow(post), size=1e4, replace = T, prob=small.post_proba)

small.sample.mu <- post$mu[small.sample.rows]
small.sample.sigma <- post$sigma[small.sample.rows]
```

```{r}
# 4.23
dens(small.sample.sigma, norm.comp = T)
```


### 4.3.5. Fitting the model with *map*

map finds the values of $\mu$ and $\sigma$ that maximize the posterior probability.
```{r}
# 4.25
model.list <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

```

```{r}
# 4.26
model.solved <- map(model.list, data=d2)
```

```{r}
# 4.27
precis(model.solved)
```

Compare to HPDI intervals from above. 

```{r}
HPDI(sample.mu)
HPDI(sample.sigma)
```

We've calculated the HPDI intervals using the grid approximation. The model is solved via a quadratic approximation. The quadratic approximation does a very good in identifying the 89% intervals. 

It works because the posterior is approximately Gaussian.

The priors we used so far are very weak. We'll splice in a more informative prior for $\mu$.

```{r}
#4.29
model.solved_narrow_mu <- map (
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1),
    sigma ~ dunif(0, 50)
  ), 
  data=d2)
precis(model.solved_narrow_mu)
```

The estimate for $\mu$ has hardly moved off the prior. The estimate for $\sigma$ has changed a lot, even though we didn't change the prior at all. Our machine had to make $\mu$ and $\sigma$ fit out data. Since $\mu$ is very concerntrated around 178, the machine had to change $\sigma$ to accomodate the data.

### 4.3.6. Sampling from a *map* fit.


Variance-covariance matrix:
```{r}
# 4.30
vcov(model.solved)
```
 We can split it into (1) vector of variances, and (2) the correlation matrix:
```{r}
# 4.31
diag(vcov(model.solved))
cov2cor(vcov(model.solved))
```
 
 Sampling from the posterior:
```{r}
# 4.34
coef(model.solved)
library(MASS)
post <- mvrnorm(n=1e4, mu=coef(model.solved), Sigma=vcov(model.solved))
post = data.frame(post)
head(post)
```
 
```{r}
# 4.33
precis(post)
```
 
```{r}
par(mfrow=c(1, 2))
smoothScatter(sample.mu, sample.sigma, cex=0.5, pch=16, col=col.alpha(rangi2, 0.1))
plot(post, col=col.alpha(rangi2, 0.1))
```
 
 
 #### Getting sigma right
 
 The quadratic assumption for $\sigma$ may be not correct. In this case it's better to estimate log($\sigma$) instead, because the distriubtion of *log* will be much closer to Guassian.
 
```{r}
# 4.35
model.solved_log_sigma <- map(
  alist(
    height ~ dnorm(mu, exp(log_sigma)),
    mu ~ dnorm(178, 20),
    log_sigma ~ dnorm(2, 10)
  ),
  data = d2
)
```
 
```{r}
# 4.36
post <- mvrnorm(n=1e4, mu=coef(model.solved_log_sigma), Sigma=vcov(model.solved_log_sigma))
post <- data.frame(post)
par(mfrow=c(1, 2))
dens(post$log_sigma)
dens(exp(post$log_sigma))
```
 
## 4.4. Adding a predictor

```{r}
#4.37
plot(d$height - d$weight)
```

### 4.4.2. Fitting the model
```{r}
# 4.38
model.linear_m43 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * weight,
    a ~ dnorm(178, 100),
    b ~ dnorm (0, 10),
    sigma ~ dunif(0, 50)
  ),
  data=d2
)
```

```{r}
# 4.40
precis(model.linear_m43, corr=T)
```

#### Centering

```{r}
# 4.42
d2$weight_centered <- d2$weight - mean(d2$weight)
```

```{r}
# 4.43
model.linear_m44 <- map(
  alist(
    height ~ dnorm(a + b * weight_centered, sigma),
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  )
  , data=d2
)
```

```{r}
# 4.44
precis(model.linear_m44, corr=T)
```

The new estimate for $\alpha$ is now the same as mean:
```{r}
mean(d2$height)
```

Let's plot the posterior against the data:
```{r}
# 4.45
plot(height ~ weight, data=d2, col="blue")
abline(a=coef(model.linear_m43)["a"], b = coef(model.linear_m43)["b"])
```
This line is just the posterior mean, the most plausible line. There are infinite regression lines from the posterior.

Let's extract some examples from the model:
```{r}
# 4.46
post <- mvrnorm(n=1e4, mu=coef(model.linear_m43), Sigma=vcov(model.linear_m43))
post <- data.frame(post)
post[1:6, ]
```

Let's try on the small data set first to see how the regression lines vary:

```{r}
# 4.48

ablines_N = function (N_) {
  library(rethinking)
  data(Howell1)
  d <- Howell1
  d2 <- d[d$age >= 18, ]
  
  dN <- d2[1:N_,]
  mN <- map(
    alist(
      height ~ dnorm(a + b * weight, sigma),
      a ~ dnorm(178, 100),
      b ~ dnorm(0, 10),
      sigma ~ dunif(0, 50)
    ), 
    data = dN
  )
  post <- mvrnorm(n=40, mu=coef(mN), Sigma=vcov(mN))
  post <- data.frame(post)
  
  plot(dN$weight, dN$height, xlim=range(d2$weight), ylim=range(d2$height), 
       col=rangi2, xlab="weight", ylab="height")
  mtext(concat("N = ", N_))
  
  for (i in 1:nrow(post))
    abline(a=post$a[i], b=post$b[i], col=col.alpha("black", 0.3))
}
```

```{r}
ablines_N(10)
```
```{r}
ablines_N(20)
```

```{r}
ablines_N(100)
```
```{r}
ablines_N(352)
```

Let's predict value for individual who weighs 91 kg:
```{r}
# 4.50
mu_at_50 <- post$a + post$b * 91
dens(mu_at_50, col=rangi2, lwd=2, xlab = "mu | weight = 91")
```

```{r}
# 4.52
HPDI(mu_at_50, prob=0.89)
```

```{r}
# 4.53
mu <- link(model.linear_m43)
str(mu)
```


Compute the distribution for each weight:
```{r}
# 4.54
weight_seq <- seq(from=25, to=100, by=1)
mu <- link(model.linear_m43, data=data.frame(weight=weight_seq))
str(mu)
```
```{r}
# 4.55
plot(height ~ weight, d2, type="n")
for (i in 1:100)
  points(weight_seq, mu[i,], pch=16, col=col.alpha(rangi2, 0.1))
```
```{r}
# 4.56
mu.mean <- apply(mu, 2, mean)
mu.hpdi <- apply(mu, 2, HPDI, prob=0.89)
mu.mean
mu.hpdi
```

Plot raw data, fading out points to make line and interval more visible:

```{r}
# 4.57
plot(height ~ weight, data=d2, col=col.alpha(rangi2, 0.5))

# Plot the MAP line, i.e. the mean mu for each weight
lines(weight_seq, mu.mean)

# Plot a shaded region for 89% HPDI
shade(mu.hpdi, weight_seq)
```

```{r}
# 4.58
post <- extract.samples(model.linear_m43)
head(post)
model.linear_m43

mu.link <- function(weight) post$a + post$b * weight

weight.seq <- seq(from=27, to=70, by=1)

mu <- sapply(weight.seq, mu.link)
head(mu)

mu.mean <- apply(mu, 2, mean)
mu.hpdi <- apply(mu, 2, HPDI, prob=0.89)
head(mu.mean)
head(mu.hpdi)

```



